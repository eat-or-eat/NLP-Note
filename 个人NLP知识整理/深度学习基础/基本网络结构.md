- [一，全连接层FCLinear — PyTorch master documentation](#一全连接层fclinear--pytorch-master-documentation)
  - [1.原理](#1原理)
  - [2.举例](#2举例)
  - [3.应用](#3应用)
  - [4.优缺点](#4优缺点)
- [二，卷积层CNNConv1d — PyTorch master documentation](#二卷积层cnnconv1d--pytorch-master-documentation)
  - [1.原理](#1原理-1)
  - [2.举例](#2举例-1)
  - [3.应用](#3应用-1)
  - [4.优缺点](#4优缺点-1)
- [三，循环神经网络RNN类](#三循环神经网络rnn类)
  - [1.原理](#1原理-2)
  - [2.举例](#2举例-2)
  - [3.应用](#3应用-2)
  - [4.优缺点](#4优缺点-2)

> 写在前面:由于个人能力和时间关系的原因，可能会有地方写错或者不太清楚，如果过路人看到欢迎指正，提issue或者加个人github微信都行，十分感谢~
>
> Pytorch官网(可供查询api、原理、使用方法):[PyTorch Hub | PyTorch](https://pytorch.org/hub/)


# 一，全连接层FC[Linear — PyTorch master documentation](https://pytorch.org/docs/master/generated/torch.nn.Linear.html?highlight=nn%20linear#torch.nn.Linear)

## 1.原理

通过$y = xA^T + b$矩阵计算方式输出新的节点

## 2.举例

$$
输入为[1,2]的连个节点，输出为[1,3]的节点 \\
input:
\begin{bmatrix}
1 & 2 
\end{bmatrix}
,W:
\begin{bmatrix}
0.1 & 0.1 & 0.1 \\
0.1 & 0.1 & 0.1 
\end{bmatrix}
B:\begin{bmatrix}1&1&1\end{bmatrix} \\
clac:
\begin{bmatrix}
1*0.1+2*0.1+1 & 1*0.1+2*0.1+1 & 1*0.1+2*0.1+1
\end{bmatrix}
= \begin{bmatrix}1.3&1.3&1.3\end{bmatrix}

$$

```python
import numpy as np

input = np.array([1, 2])
weight = np.full([2, 3], 0.1)
b = np.full([1, 3], 1)


def linear(input, weight, b):
    return np.dot(input, weight) + b


output = linear(input, weight, b)
```

## 3.应用

常用于神经网络最后所需预测类别或数值的一层(分类问题或回归问题最后一层)，或每个小结构的最后一层做数据维度整理(transformer中的FFN模块)，或attention计算(self-attention的QK相乘得到[句长,句长]的矩阵)


## 4.优缺点

优点：

1. 方便使用
2. 能根据前一层的所有特征进行学习

缺点：

1. 参数量大，尤其在图像中，特征提取多用卷积网络替代
2. 层数多了容易过拟合，因为反向传播时层数越多前层的权值修改幅度就会越小

# 二，卷积层CNN[Conv1d — PyTorch master documentation](https://pytorch.org/docs/master/generated/torch.nn.Conv1d.html?highlight=nn%20conv1d#torch.nn.Conv1d)

## 1.原理

有1D，2D，3D卷积三种卷积，通过卷积核来进行局部特征提取，然后配合步长对全数据遍历，可能有些情况会添加padding以改变后续层的shape，之后还可以串联一个池化层来减少参数

## 2.举例

将一个文本数据进行conv1d提取特征

```python
import torch
import torch.nn as nn

conv1d = nn.Conv1d(128, 32, 3, stride=1)  # 将128维的词嵌入向量提取出32维的隐含层
input = torch.randn(64, 128, 30)  # 模拟(bs,emb_size,seq_len)为[64,128,30]
output = conv1d(input)

print(output.shape)
# torch.Size([64, 32, 28])  # 此时的32维隐含层是每个句子都提取出一个特征，有32个卷积核
```


最大池化：

```python
[[1,2],
[3,4]]
->4
```

平均池化：

```python
[[1,2],
[3,4]]
->2.5
```

## 3.应用

常用在1d常用在文字特征提取(TextCNN),2d常用在图像的特征提取(VGG),3D常用在视频提取(Spatiotemporal 3D CNNs)

## 4.优缺点

优点：

1. 相对于全连接减少了很多参数，因为卷积核参数共享
2. 局部感知能获取部分信息整合，同时多卷积核能学到不同的特征，原因是卷积核的参数是随机初始化的，不同的初始化可以将权重学到不同的值

缺点：

1. 多层网络串联同样会导致前层的权重更新幅度很小
2. 池化层的选取需要根据经验，比如TextCNN分类中大概率用MaxPool比AvgPool好，因为某些字词可能对最终的分类结果影响较大

# 三，循环神经网络RNN类

## 1.原理

通过隐藏状态和串行输入信息进行学习，$h_t = \tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})$

## 2.举例

```python
import torch
import torch.nn as nn

rnn = nn.RNN(128, 256, batch_first=True)  # 将128维的词嵌入变成256维的隐含层
input = torch.randn(64, 30, 128)  # 模拟(bs,seq_len,emb_size)
output, hn = rnn(input)

print(output.shape)
# torch.Size([64, 30, 256])
```

## 3.应用

常用于时序或者文本的网络结构中，RNN、LSTM、GRU等模型做编码器解码器或特征提取器

## 4.优缺点

优点：

1. 能学习到时序特征

缺点：

1. RNN容易造成梯度消失而且忽略长举例特征，可以用LSTM进行缓解
2. LSTM训练速度慢，可以用GRU进行缓解
3. 单向时序模型无法学到上下前后特征，可以通过BiRnn类型网络或者transformor结构进行缓解
